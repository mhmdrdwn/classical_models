{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26f46c8-2a8a-4a73-9308-d435502bd326",
   "metadata": {},
   "source": [
    "# ChronoNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666faf2-30e3-4ce8-a625-f3c98327fe5a",
   "metadata": {},
   "source": [
    "Here we train the model and record the train and validation accuracy. We used subset of the temple dataset for faster training. we used 500 subjects only for training and validation of the model. The recorded metrics are accuracy, binary cross entropy loss, F1 score, Precision and Recall.\n",
    "\n",
    "- Train Data: 500 subjects\n",
    "\n",
    "- Test Data: 276 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d47023-733f-4137-a16d-a08406cefc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedr/opt/anaconda3/envs/xai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data....\n",
      "Scaling Data....\n",
      "Data Loader....\n",
      "Training Model....\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [04:37<00:00,  2.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6185583529100253 ,Train Accuracy:  0.7934802259887006 ,F1:  0.7612441378949981 ,Precision:  0.9020463762731804 ,Recall:  0.6584632768361582\n",
      "Val loss: 0.6317908183320776 , Val Accuracy:  0.6661016949152543 ,F1:  0.5159229408295657 ,Precision:  0.9376563058235085 ,Recall:  0.355864406779661\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [03:47<00:00,  3.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5862326041466928 ,Train Accuracy:  0.7856497175141243 ,F1:  0.7381642512077294 ,Precision:  0.9482269503546099 ,Recall:  0.6042937853107344\n",
      "Val loss: 0.6265238835698083 , Val Accuracy:  0.6772542372881356 ,F1:  0.5373438942611399 ,Precision:  0.9485331960885229 ,Recall:  0.37484745762711863\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [03:43<00:00,  3.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5769695649946356 ,Train Accuracy:  0.8252994350282485 ,F1:  0.7993771491598001 ,Precision:  0.9386561023921987 ,Recall:  0.696090395480226\n",
      "Val loss: 0.611729929715524 , Val Accuracy:  0.7238305084745763 ,F1:  0.6365055994289028 ,Precision:  0.9308364870155291 ,Recall:  0.48359322033898305\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [03:44<00:00,  3.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5716010270091151 ,Train Accuracy:  0.8470621468926554 ,F1:  0.8297291517278687 ,Precision:  0.935785023126472 ,Recall:  0.7452655367231639\n",
      "Val loss: 0.607604176967175 , Val Accuracy:  0.7368135593220339 ,F1:  0.6608125819134995 ,Precision:  0.9291154791154791 ,Recall:  0.5127457627118645\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [03:53<00:00,  2.97it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5678945554990988 ,Train Accuracy:  0.8633559322033898 ,F1:  0.8515376588300289 ,Precision:  0.9321596559602204 ,Recall:  0.7837514124293785\n",
      "Val loss: 0.6037838544164386 , Val Accuracy:  0.7559661016949153 ,F1:  0.7058150463814311 ,Precision:  0.888385968521757 ,Recall:  0.5854915254237288\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from load_data import read_data_arrays, data_file_names, standardize_data, data_loader\n",
    "from models import ChronoNet\n",
    "from utils import cal_accuracy, evaluate_model \n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "print(\"Reading Data....\")\n",
    "sample_size = 500\n",
    "data_files = data_file_names(sample_size)\n",
    "(train_features, val_features, test_features,\n",
    " train_labels, val_labels, test_labels, test_lengths) = read_data_arrays(\n",
    "    data_files)\n",
    "    \n",
    "print(\"Scaling Data....\")\n",
    "train_features, val_features, test_features = standardize_data(\n",
    "    train_features, val_features, test_features)\n",
    "    \n",
    "print(\"Data Loader....\")\n",
    "train_iter = data_loader(train_features, train_labels, DEVICE, BATCH_SIZE)\n",
    "val_iter = data_loader(val_features, val_labels, DEVICE, BATCH_SIZE)\n",
    "test_iter = data_loader(test_features, test_labels, DEVICE, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "print(\"Training Model....\")\n",
    "n_chans = 19\n",
    "model=ChronoNet(n_chans)\n",
    "model.to(DEVICE)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(\"Epoch\", epoch) \n",
    "    loss_sum, n = 0.0, 0\n",
    "    model.train()\n",
    "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    val_loss = evaluate_model(model, loss_func, val_iter)\n",
    "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
    "        cal_accuracy(model, train_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, train_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, train_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, train_iter)[3])\n",
    "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
    "        cal_accuracy(model, val_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, val_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, val_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, val_iter)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4ec1d-fbd4-4d0f-9173-e2ae3534691d",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7220e28-acda-411d-9eec-2c5f22572cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8043478260869565\n",
      "Test Confusion:  [[141   9]\n",
      " [ 45  81]]\n",
      "Test F1:  0.75\n",
      "Test Recall:  0.6428571428571429\n",
      "Test Precision:  0.9\n"
     ]
    }
   ],
   "source": [
    "ytrue = []\n",
    "ypreds = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_iter:\n",
    "        yhat = model(x)\n",
    "        yhat = [0 if i<0.5 else 1 for i in yhat]\n",
    "        ytrue.extend(list(y.numpy()))\n",
    "        ypreds.extend(yhat)\n",
    "        \n",
    "from collections import Counter\n",
    "\n",
    "y_final = []\n",
    "yhat_final = []\n",
    "for i in range(0, len(ytrue), 118): \n",
    "    major_y_final = Counter(ytrue[i: i+118]).most_common(1)[0][0]\n",
    "    major_yhat_final = Counter(ypreds[i: i+118]).most_common(1)[0][0]\n",
    "    y_final.append(major_y_final)\n",
    "    yhat_final.append(major_yhat_final)\n",
    "    \n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "print(\"Test Accuracy: \", accuracy_score(y_final, yhat_final))\n",
    "print(\"Test Confusion: \",confusion_matrix(y_final, yhat_final))\n",
    "print(\"Test F1: \", f1_score(y_final, yhat_final))\n",
    "print(\"Test Recall: \", recall_score(y_final, yhat_final))\n",
    "print(\"Test Precision: \", precision_score(y_final, yhat_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619dde7-c736-40dc-a86e-b8dad07c2809",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254afb0b-0902-4330-bc50-14afd53623a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model....\n",
      "started training\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [08:51<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.661770528279288 ,Train Accuracy:  0.7018870056497175 ,F1:  0.624489389259739 ,Precision:  0.843477257872275 ,Recall:  0.49577401129943505\n",
      "Val loss: 0.67322963032068 , Val Accuracy:  0.5675593220338983 ,F1:  0.2789803877239586 ,Precision:  0.8386000679578661 ,Recall:  0.16732203389830508\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [36:10<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6248360861071273 ,Train Accuracy:  0.7378757062146892 ,F1:  0.6679738936279842 ,Precision:  0.9108829729096729 ,Recall:  0.5273446327683616\n",
      "Val loss: 0.6709002801016265 , Val Accuracy:  0.5743050847457627 ,F1:  0.2901074053137365 ,Precision:  0.8727891156462585 ,Recall:  0.17396610169491525\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [13:54<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6036012842820558 ,Train Accuracy:  0.7931073446327683 ,F1:  0.7555798803929944 ,Precision:  0.9229991520448764 ,Recall:  0.6395706214689265\n",
      "Val loss: 0.6608413919514301 , Val Accuracy:  0.602 ,F1:  0.3752461022721226 ,Precision:  0.8721246599060104 ,Recall:  0.2390508474576271\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [13:37<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5900325086075446 ,Train Accuracy:  0.8101694915254237 ,F1:  0.777824798984342 ,Precision:  0.937575718931327 ,Recall:  0.6645875706214689\n",
      "Val loss: 0.6607672583823111 , Val Accuracy:  0.604 ,F1:  0.37907940895078135 ,Precision:  0.8774606299212598 ,Recall:  0.2417627118644068\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [17:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5812325871231928 ,Train Accuracy:  0.833683615819209 ,F1:  0.8115485564304462 ,Precision:  0.9361394181066313 ,Recall:  0.716225988700565\n",
      "Val loss: 0.6539293941329507 , Val Accuracy:  0.6249830508474576 ,F1:  0.44049967126890205 ,Precision:  0.8670117459685447 ,Recall:  0.2952542372881356\n"
     ]
    }
   ],
   "source": [
    "from models import LSTM\n",
    "\n",
    "learning_rate = 5e-3\n",
    "\n",
    "model = LSTM(input_size=500, num_channels=19, hidden_units=128)\n",
    "\n",
    "print(\"Training Model....\")\n",
    "n_chans = 19\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model.to(DEVICE)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print(\"started training\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(\"Epoch\", epoch) \n",
    "    loss_sum, n = 0.0, 0\n",
    "    model.train()\n",
    "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    val_loss = evaluate_model(model, loss_func, test_iter)\n",
    "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
    "        cal_accuracy(model, train_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, train_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, train_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, train_iter)[3])\n",
    "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
    "        cal_accuracy(model, val_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, val_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, val_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, val_iter)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82189487-7315-4a8c-b73f-f60d7c24c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.6485507246376812\n",
      "Test Confusion:  [[149   1]\n",
      " [ 96  30]]\n",
      "Test F1:  0.38216560509554137\n",
      "Test Recall:  0.23809523809523808\n",
      "Test Precision:  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "ytrue = []\n",
    "ypreds = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_iter:\n",
    "        yhat = model(x)\n",
    "        yhat = [0 if i<0.5 else 1 for i in yhat]\n",
    "        ytrue.extend(list(y.numpy()))\n",
    "        ypreds.extend(yhat)\n",
    "\n",
    "y_final = []\n",
    "yhat_final = []\n",
    "for i in range(0, len(ytrue), 118): \n",
    "    major_y_final = Counter(ytrue[i: i+118]).most_common(1)[0][0]\n",
    "    major_yhat_final = Counter(ypreds[i: i+118]).most_common(1)[0][0]\n",
    "    y_final.append(major_y_final)\n",
    "    yhat_final.append(major_yhat_final)\n",
    "    \n",
    "print(\"Test Accuracy: \", accuracy_score(y_final, yhat_final))\n",
    "print(\"Test Confusion: \",confusion_matrix(y_final, yhat_final))\n",
    "print(\"Test F1: \", f1_score(y_final, yhat_final))\n",
    "print(\"Test Recall: \", recall_score(y_final, yhat_final))\n",
    "print(\"Test Precision: \", precision_score(y_final, yhat_final))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
