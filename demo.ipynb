{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2f7bb62f",
   "metadata": {},
   "source": [
    "## Read Labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e37ed9d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "labels = pd.read_csv(\"nmt_scalp_eeg_dataset/Labels.csv\")\n",
    "labels_dict = {}\n",
    "\n",
    "subjects = labels.recordname\n",
    "labels = labels.label\n",
    "labels = [0 if l.lower() == \"normal\" else 1 for l in labels]\n",
    "\n",
    "for sub, label in zip(subjects, labels):\n",
    "    labels_dict[sub] = label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e613328f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34meval\u001b[m\u001b[m  \u001b[34mtrain\u001b[m\u001b[m\n"
     ]
    }
   ],
   "source": [
    "!ls nmt_scalp_eeg_dataset/normal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8e4ec283",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels_dict['0000001.edf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0d1b14e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2417"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of subjects\n",
    "len(subjects)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b3fadf",
   "metadata": {},
   "source": [
    "## Read edf data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "74354c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xai\n"
     ]
    }
   ],
   "source": [
    "!echo $CONDA_DEFAULT_ENV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b77b8ca6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "import scipy.io\n",
    "\n",
    "import os\n",
    "import mne\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "726678a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(file_path):\n",
    "    datax=mne.io.read_raw_edf(file_path,preload=True)\n",
    "    datax.set_eeg_reference()\n",
    "    datax.filter(l_freq=1,h_freq=45)\n",
    "    epochs=mne.make_fixed_length_epochs(datax,duration=10,overlap=0)\n",
    "    epochs=(epochs.get_data() * 1e6).astype(np.float32)\n",
    "    return epochs #trials,channel,length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "9e1ddc69-4d4a-4979-99dd-b9498103d3b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_abnormal_files = glob.glob('./nmt_scalp_eeg_dataset/abnormal/eval/*.edf')\n",
    "test_normal_files = glob.glob('./nmt_scalp_eeg_dataset/normal/eval/*.edf')\n",
    "train_normal_files = glob.glob('./nmt_scalp_eeg_dataset/normal/train/*.edf')\n",
    "train_abnormal_files = glob.glob('./nmt_scalp_eeg_dataset/abnormal/train/*.edf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "0999cde3-1b1a-41fd-92c6-d1d1aaa5c7a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "val_normal_files = random.sample(train_normal_files, 10)\n",
    "val_abnormal_files = random.sample(train_abnormal_files, 10)\n",
    "train_normal_files = random.sample(train_normal_files, 50)\n",
    "train_abnormal_files = random.sample(train_abnormal_files, 50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "dafdac53-9353-453d-a318-1a5d9611c0d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 50)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_abnormal_files), len(train_normal_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "c846b936",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "\n",
    "train_normal_features=[load_data(f) for f in train_normal_files]\n",
    "train_abnormal_features=[load_data(f) for f in train_abnormal_files]\n",
    "train_normal_labels=[0 for f in train_normal_files]\n",
    "train_abnormal_labels=[1 for f in train_abnormal_files]\n",
    "\n",
    "val_normal_features=[load_data(f) for f in val_normal_files]\n",
    "val_abnormal_features=[load_data(f) for f in val_abnormal_files]\n",
    "val_normal_labels=[0 for f in val_normal_files]\n",
    "val_abnormal_labels=[1 for f in val_abnormal_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "05d9173e-c1e1-4722-a648-c71f23845e58",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_normal_labels=[len(i)*[0] for i in train_normal_features]\n",
    "train_abnormal_labels=[len(i)*[1] for i in train_abnormal_features]\n",
    "\n",
    "val_normal_labels=[len(i)*[0] for i in val_normal_features]\n",
    "val_abnormal_labels=[len(i)*[1] for i in val_abnormal_features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "2e929e2b-7457-4e12-8c0b-eb7d837649b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = train_normal_features + train_abnormal_features\n",
    "train_labels = train_normal_labels + train_abnormal_labels\n",
    "\n",
    "val_features = val_normal_features + val_abnormal_features\n",
    "val_labels = val_normal_labels + val_abnormal_labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "625b5c0d-6a06-418e-8791-1714ebfbaebb",
   "metadata": {},
   "outputs": [],
   "source": [
    "del train_normal_features\n",
    "del train_abnormal_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "89dd9e61-083e-4d89-8ccc-a55984dba6b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_features = np.vstack(train_features)\n",
    "train_labels = np.hstack(train_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ee677aef-ca42-4daf-9fac-b3d0c72b6ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "val_features = np.vstack(val_features)\n",
    "val_labels = np.hstack(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "249ab5bc-608a-464e-86f1-2f88bc578fd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from sklearn.model_selection import GroupKFold\n",
    "gkf=GroupKFold()\n",
    "from sklearn.base import TransformerMixin,BaseEstimator\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "#https://stackoverflow.com/questions/50125844/how-to-standard-scale-a-3d-matrix\n",
    "class StandardScaler3D(BaseEstimator,TransformerMixin):\n",
    "    #batch, sequence, channels\n",
    "    def __init__(self):\n",
    "        self.scaler = StandardScaler()\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        self.scaler.fit(X.reshape(-1, X.shape[2]))\n",
    "        return self\n",
    "\n",
    "    def transform(self,X):\n",
    "        return self.scaler.transform(X.reshape( -1,X.shape[2])).reshape(X.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "3afd2c7a-0469-4c57-8496-41ab5c14bb6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler3D()\n",
    "train_features = scaler.fit_transform(train_features)\n",
    "val_features = scaler.transform(val_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "b0f09db8-1c0f-4c40-920a-85d7af073b24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "2047317b-daaf-4cad-bb2a-807e54c0989f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self,inplace):\n",
    "    super().__init__()\n",
    "    self.conv1=nn.Conv1d(in_channels=inplace,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "    self.conv2=nn.Conv1d(in_channels=inplace,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "    self.conv3=nn.Conv1d(in_channels=inplace,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    x1=self.relu(self.conv1(x))\n",
    "    x2=self.relu(self.conv2(x))\n",
    "    x3=self.relu(self.conv3(x))\n",
    "    x=torch.cat([x1,x3,x3],dim=1)\n",
    "    return x\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "  def __init__(self,channel):\n",
    "    super().__init__()\n",
    "    self.block1=Block(channel)\n",
    "    self.block2=Block(96)\n",
    "    self.block3=Block(96)\n",
    "    self.gru1=nn.GRU(input_size=96,hidden_size=32,batch_first=True)\n",
    "    self.gru2=nn.GRU(input_size=32,hidden_size=32,batch_first=True)\n",
    "    self.gru3=nn.GRU(input_size=64,hidden_size=32,batch_first=True)\n",
    "    self.gru4=nn.GRU(input_size=96,hidden_size=32,batch_first=True)\n",
    "    self.gru_linear=nn.Linear(250,1)\n",
    "    self.flatten=nn.Flatten()\n",
    "    self.fc1=nn.Linear(32,1)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x.squeeze()\n",
    "    x=self.block1(x)\n",
    "    x=self.block2(x)\n",
    "    x=self.block3(x)\n",
    "    x=x.permute(0,2,1)\n",
    "    gru_out1,_=self.gru1(x)\n",
    "    gru_out2,_=self.gru2(gru_out1)\n",
    "    gru_out=torch.cat([gru_out1,gru_out2],dim=2)\n",
    "    gru_out3,_=self.gru3(gru_out)\n",
    "    gru_out=torch.cat([gru_out1,gru_out2,gru_out3],dim=2)\n",
    "    linear_out=self.relu(self.gru_linear(gru_out.permute(0,2,1)))\n",
    "    gru_out4,_=self.gru4(linear_out.permute(0,2,1))\n",
    "    x=self.flatten(gru_out4)\n",
    "    x=self.fc1(x)\n",
    "    out = torch.sigmoid(x)\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "a20d5089-2d78-4c39-90d7-1e5de94b9ad4",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\") if torch.cuda.is_available() else torch.device(\"cpu\")\n",
    "\n",
    "train_features = torch.Tensor(train_features).to(device)\n",
    "train_labels = torch.Tensor(train_labels).to(device)\n",
    "\n",
    "batch_size = 16\n",
    "train_data = torch.utils.data.TensorDataset(train_features, train_labels)\n",
    "train_iter = torch.utils.data.DataLoader(train_data, batch_size, shuffle=True)\n",
    "\n",
    "val_features = torch.Tensor(val_features).to(device)\n",
    "val_labels = torch.Tensor(val_labels).to(device)\n",
    "\n",
    "batch_size = 16\n",
    "val_data = torch.utils.data.TensorDataset(val_features, val_labels)\n",
    "val_iter = torch.utils.data.DataLoader(val_data, batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "75ae0198-2fe5-490e-bb5c-548ae0119871",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([21, 2000])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "823dd9f3-83ed-4ba1-bd94-a982c02fe8a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, loss_func, data_iter):\n",
    "    model.eval()\n",
    "    loss_sum, n = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for x, y in data_iter:\n",
    "            y_pred = model(x)\n",
    "            y_pred = y_pred.squeeze()\n",
    "            loss = loss_func(y_pred,y)\n",
    "            loss_sum += loss.item()\n",
    "            n += 1\n",
    "        return loss_sum / n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "e569ea71-7b1e-4edd-8f1a-7a3bd7d7b5a3",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:43<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6859325429126447\n",
      "Val loss: 0.6846961152553558\n",
      "epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:43<00:00,  4.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6660683229047019\n",
      "Val loss: 0.6759910082817078\n",
      "epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:45<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6682151165334919\n",
      "Val loss: 0.630389387011528\n",
      "epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:44<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6234215364517026\n",
      "Val loss: 0.6270960444211959\n",
      "epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:44<00:00,  4.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6106176890795856\n",
      "Val loss: 0.6167118436098099\n",
      "epoch 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:44<00:00,  4.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6031776645897159\n",
      "Val loss: 0.6060078346729278\n",
      "epoch 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:43<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6017707470813096\n",
      "Val loss: 0.6176279726624488\n",
      "epoch 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 431/431 [01:43<00:00,  4.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5959621176918809\n",
      "Val loss: 0.6084504339098931\n",
      "epoch 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 29%|███████████▉                             | 125/431 [00:30<01:14,  4.10it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [66]\u001b[0m, in \u001b[0;36m<cell line: 7>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m y_pred\u001b[38;5;241m.\u001b[39msqueeze()\n\u001b[1;32m     14\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_func(y_pred, y)\n\u001b[0;32m---> 15\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m loss_sum \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[1;32m     17\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xai/lib/python3.10/site-packages/torch/_tensor.py:396\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    387\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    388\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    389\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    390\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    394\u001b[0m         create_graph\u001b[38;5;241m=\u001b[39mcreate_graph,\n\u001b[1;32m    395\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs)\n\u001b[0;32m--> 396\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/envs/xai/lib/python3.10/site-packages/torch/autograd/__init__.py:173\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    168\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    170\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[1;32m    171\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    172\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 173\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    174\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    175\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "n_chans = 21\n",
    "model=ChronoNet(n_chans)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(1, epochs + 1):\n",
    "    print(\"epoch\", epoch) \n",
    "    loss_sum, n = 0.0, 0\n",
    "    model.train()\n",
    "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    val_loss = evaluate_model(model, loss_func, val_iter)\n",
    "    print(\"Train loss:\", loss_sum / (t+1))\n",
    "    print(\"Val loss:\", val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5b3ba24-2709-4b3d-9878-082ea9524ab9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.634364660177581"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "loss_sum / (t+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "96d47023-733f-4137-a16d-a08406cefc6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy:  0.70825456836799\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    y_hat = model(val_features)\n",
    "\n",
    "yhat = [0 if i<0.5 else 1 for i in y_hat]\n",
    "ytrue = val_labels.numpy()\n",
    "ypreds = yhat\n",
    "\n",
    "from sklearn.metrics import accuracy_score\n",
    "print(\"Accuracy: \", accuracy_score(ytrue, ypreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "3fc4e58d-23d4-447b-adb5-e14c8b896739",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[616, 112],\n",
       "       [351, 508]])"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "confusion_matrix(ytrue, ypreds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3505316-f5ea-4783-a12c-5f712c1bddd9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
