{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f26f46c8-2a8a-4a73-9308-d435502bd326",
   "metadata": {},
   "source": [
    "# ChronoNet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4666faf2-30e3-4ce8-a625-f3c98327fe5a",
   "metadata": {},
   "source": [
    "Here we train the model and record the train and validation accuracy. We used subset of the temple dataset for faster training. we used 500 subjects only for training and validation of the model. The recorded metrics are accuracy, binary cross entropy loss, F1 score, Precision and Recall.\n",
    "\n",
    "- Train Data: 500 subjects\n",
    "\n",
    "- Test Data: 276 subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "99194e02-6f9f-4609-8e16-600c35eaddf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1371\n"
     ]
    }
   ],
   "source": [
    "!ls tuh/train/normal/* | wc -l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "96d47023-733f-4137-a16d-a08406cefc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/mohamedr/opt/anaconda3/envs/xai/lib/python3.10/site-packages/tqdm/auto.py:22: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading Data....\n",
      "Scaling Data....\n",
      "Data Loader....\n",
      "Training Model....\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [04:34<00:00,  2.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6135080639337529 ,Train Accuracy:  0.7996045197740113 ,F1:  0.7803633571525878 ,Precision:  0.8632490341671918 ,Recall:  0.712\n",
      "Val loss: 0.6022120368429077 , Val Accuracy:  0.7723050847457628 ,F1:  0.741564387672656 ,Precision:  0.8573080686771639 ,Recall:  0.6533559322033898\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [04:21<00:00,  2.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5856635680777489 ,Train Accuracy:  0.8226892655367232 ,F1:  0.7964404317144043 ,Precision:  0.934831597539436 ,Recall:  0.6937401129943502\n",
      "Val loss: 0.5953799966093781 , Val Accuracy:  0.7829830508474577 ,F1:  0.7506232471174821 ,Precision:  0.8821644387474822 ,Recall:  0.6532203389830509\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [04:45<00:00,  2.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5753913293511881 ,Train Accuracy:  0.8453446327683616 ,F1:  0.8308555469049296 ,Precision:  0.9167416618942431 ,Recall:  0.7596836158192091\n",
      "Val loss: 0.5960082494335257 , Val Accuracy:  0.792 ,F1:  0.7715221924337206 ,Precision:  0.8557739963654386 ,Recall:  0.7023728813559322\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [04:30<00:00,  2.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.570163000353499 ,Train Accuracy:  0.8605762711864406 ,F1:  0.8510484192228297 ,Precision:  0.9134727513021845 ,Recall:  0.7966101694915254\n",
      "Val loss: 0.595359214972624 , Val Accuracy:  0.7965084745762712 ,F1:  0.7804717498628635 ,Precision:  0.8472409686383485 ,Recall:  0.723457627118644\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [04:15<00:00,  2.71it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.565777277498576 ,Train Accuracy:  0.8552542372881355 ,F1:  0.8381267691063485 ,Precision:  0.9506105601100728 ,Recall:  0.7494463276836159\n",
      "Val loss: 0.5963385198023412 , Val Accuracy:  0.7821016949152543 ,F1:  0.7513346228239846 ,Precision:  0.8748648648648648 ,Recall:  0.6583728813559322\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "\n",
    "from load_data import read_data_arrays, data_file_names, standardize_data, data_loader\n",
    "from models import ChronoNet\n",
    "from utils import cal_accuracy, evaluate_model \n",
    "\n",
    "\n",
    "BATCH_SIZE = 128\n",
    "#device = torch.device(\"mps\") if torch.backends.mps.is_available() else torch.device(\"cpu\")\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "NUM_EPOCHS = 5\n",
    "\n",
    "print(\"Reading Data....\")\n",
    "sample_size = 500\n",
    "data_files = data_file_names(sample_size)\n",
    "(train_features, val_features, test_features,\n",
    " train_labels, val_labels, test_labels, test_lengths) = read_data_arrays(\n",
    "    data_files)\n",
    "    \n",
    "print(\"Scaling Data....\")\n",
    "train_features, val_features, test_features = standardize_data(\n",
    "    train_features, val_features, test_features)\n",
    "    \n",
    "print(\"Data Loader....\")\n",
    "train_iter = data_loader(train_features, train_labels, DEVICE, BATCH_SIZE)\n",
    "val_iter = data_loader(val_features, val_labels, DEVICE, BATCH_SIZE)\n",
    "test_iter = data_loader(test_features, test_labels, DEVICE, BATCH_SIZE, shuffle=False)\n",
    "    \n",
    "print(\"Training Model....\")\n",
    "n_chans = 19\n",
    "model=ChronoNet(n_chans)\n",
    "model.to(DEVICE)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(\"Epoch\", epoch) \n",
    "    loss_sum, n = 0.0, 0\n",
    "    model.train()\n",
    "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    val_loss = evaluate_model(model, loss_func, val_iter)\n",
    "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
    "        cal_accuracy(model, train_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, train_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, train_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, train_iter)[3])\n",
    "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
    "        cal_accuracy(model, val_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, val_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, val_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, val_iter)[3])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b4ec1d-fbd4-4d0f-9173-e2ae3534691d",
   "metadata": {},
   "source": [
    "## Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a7220e28-acda-411d-9eec-2c5f22572cb0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.8188405797101449\n",
      "Test Confusion:  [[141   9]\n",
      " [ 41  85]]\n",
      "Test F1:  0.7727272727272727\n",
      "Test Recall:  0.6746031746031746\n",
      "Test Precision:  0.9042553191489362\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "from sklearn.metrics import accuracy_score, confusion_matrix, f1_score, precision_score, recall_score\n",
    "\n",
    "ytrue = []\n",
    "ypreds = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_iter:\n",
    "        yhat = model(x)\n",
    "        yhat = [0 if i<0.5 else 1 for i in yhat]\n",
    "        ytrue.extend(list(y.numpy()))\n",
    "        ypreds.extend(yhat)\n",
    "        \n",
    "y_final = []\n",
    "yhat_final = []\n",
    "for i in range(0, len(ytrue), 118): \n",
    "    major_y_final = Counter(ytrue[i: i+118]).most_common(1)[0][0]\n",
    "    major_yhat_final = Counter(ypreds[i: i+118]).most_common(1)[0][0]\n",
    "    y_final.append(major_y_final)\n",
    "    yhat_final.append(major_yhat_final)\n",
    "    \n",
    "print(\"Test Accuracy: \", accuracy_score(y_final, yhat_final))\n",
    "print(\"Test Confusion: \",confusion_matrix(y_final, yhat_final))\n",
    "print(\"Test F1: \", f1_score(y_final, yhat_final))\n",
    "print(\"Test Recall: \", recall_score(y_final, yhat_final))\n",
    "print(\"Test Precision: \", precision_score(y_final, yhat_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9619dde7-c736-40dc-a86e-b8dad07c2809",
   "metadata": {},
   "source": [
    "# LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "254afb0b-0902-4330-bc50-14afd53623a5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model....\n",
      "started training\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [08:51<00:00,  1.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.661770528279288 ,Train Accuracy:  0.7018870056497175 ,F1:  0.624489389259739 ,Precision:  0.843477257872275 ,Recall:  0.49577401129943505\n",
      "Val loss: 0.67322963032068 , Val Accuracy:  0.5675593220338983 ,F1:  0.2789803877239586 ,Precision:  0.8386000679578661 ,Recall:  0.16732203389830508\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [36:10<00:00,  3.14s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6248360861071273 ,Train Accuracy:  0.7378757062146892 ,F1:  0.6679738936279842 ,Precision:  0.9108829729096729 ,Recall:  0.5273446327683616\n",
      "Val loss: 0.6709002801016265 , Val Accuracy:  0.5743050847457627 ,F1:  0.2901074053137365 ,Precision:  0.8727891156462585 ,Recall:  0.17396610169491525\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [13:54<00:00,  1.21s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6036012842820558 ,Train Accuracy:  0.7931073446327683 ,F1:  0.7555798803929944 ,Precision:  0.9229991520448764 ,Recall:  0.6395706214689265\n",
      "Val loss: 0.6608413919514301 , Val Accuracy:  0.602 ,F1:  0.3752461022721226 ,Precision:  0.8721246599060104 ,Recall:  0.2390508474576271\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [13:37<00:00,  1.18s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5900325086075446 ,Train Accuracy:  0.8101694915254237 ,F1:  0.777824798984342 ,Precision:  0.937575718931327 ,Recall:  0.6645875706214689\n",
      "Val loss: 0.6607672583823111 , Val Accuracy:  0.604 ,F1:  0.37907940895078135 ,Precision:  0.8774606299212598 ,Recall:  0.2417627118644068\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [17:01<00:00,  1.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5812325871231928 ,Train Accuracy:  0.833683615819209 ,F1:  0.8115485564304462 ,Precision:  0.9361394181066313 ,Recall:  0.716225988700565\n",
      "Val loss: 0.6539293941329507 , Val Accuracy:  0.6249830508474576 ,F1:  0.44049967126890205 ,Precision:  0.8670117459685447 ,Recall:  0.2952542372881356\n"
     ]
    }
   ],
   "source": [
    "from models import LSTM\n",
    "\n",
    "learning_rate = 5e-3\n",
    "\n",
    "model = LSTM(input_size=500, num_channels=19, hidden_units=128)\n",
    "\n",
    "print(\"Training Model....\")\n",
    "n_chans = 19\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "model.to(DEVICE)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print(\"started training\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(\"Epoch\", epoch) \n",
    "    loss_sum, n = 0.0, 0\n",
    "    model.train()\n",
    "    for t, (x, y) in enumerate(tqdm(train_iter)):\n",
    "        y_pred = model(x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    val_loss = evaluate_model(model, loss_func, test_iter)\n",
    "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
    "        cal_accuracy(model, train_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, train_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, train_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, train_iter)[3])\n",
    "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
    "        cal_accuracy(model, val_iter)[0], \",F1: \", \n",
    "        cal_accuracy(model, val_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(model, val_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(model, val_iter)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "82189487-7315-4a8c-b73f-f60d7c24c742",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Accuracy:  0.6485507246376812\n",
      "Test Confusion:  [[149   1]\n",
      " [ 96  30]]\n",
      "Test F1:  0.38216560509554137\n",
      "Test Recall:  0.23809523809523808\n",
      "Test Precision:  0.967741935483871\n"
     ]
    }
   ],
   "source": [
    "ytrue = []\n",
    "ypreds = []\n",
    "with torch.no_grad():\n",
    "    for x, y in test_iter:\n",
    "        yhat = model(x)\n",
    "        yhat = [0 if i<0.5 else 1 for i in yhat]\n",
    "        ytrue.extend(list(y.numpy()))\n",
    "        ypreds.extend(yhat)\n",
    "\n",
    "y_final = []\n",
    "yhat_final = []\n",
    "for i in range(0, len(ytrue), 118): \n",
    "    major_y_final = Counter(ytrue[i: i+118]).most_common(1)[0][0]\n",
    "    major_yhat_final = Counter(ypreds[i: i+118]).most_common(1)[0][0]\n",
    "    y_final.append(major_y_final)\n",
    "    yhat_final.append(major_yhat_final)\n",
    "    \n",
    "print(\"Test Accuracy: \", accuracy_score(y_final, yhat_final))\n",
    "print(\"Test Confusion: \",confusion_matrix(y_final, yhat_final))\n",
    "print(\"Test F1: \", f1_score(y_final, yhat_final))\n",
    "print(\"Test Recall: \", recall_score(y_final, yhat_final))\n",
    "print(\"Test Precision: \", precision_score(y_final, yhat_final))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "272ba1b7-5469-4d4d-b942-2e97c64fde87",
   "metadata": {},
   "source": [
    "# GCN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "id": "af0f36fc-045e-4a3e-9834-aa6a1836ce39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "import torch\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "from torch.nn.modules.module import Module\n",
    "\n",
    "from tqdm import tqdm\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "\n",
    "class Block(nn.Module):\n",
    "  def __init__(self,inplace):\n",
    "    super().__init__()\n",
    "    self.conv1=nn.Conv1d(in_channels=inplace,out_channels=32,kernel_size=2,stride=2,padding=0)\n",
    "    self.conv2=nn.Conv1d(in_channels=inplace,out_channels=32,kernel_size=4,stride=2,padding=1)\n",
    "    self.conv3=nn.Conv1d(in_channels=inplace,out_channels=32,kernel_size=8,stride=2,padding=3)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    x1=self.relu(self.conv1(x))\n",
    "    x2=self.relu(self.conv2(x))\n",
    "    x3=self.relu(self.conv3(x))\n",
    "    x=torch.cat([x1,x3,x3],dim=1)\n",
    "    return x\n",
    "\n",
    "class ChronoNet(nn.Module):\n",
    "  def __init__(self,channel):\n",
    "    super().__init__()\n",
    "    self.block1=Block(channel)\n",
    "    self.block2=Block(96)\n",
    "    self.block3=Block(96)\n",
    "    self.gru1=nn.LSTM(input_size=96,hidden_size=32,batch_first=True)\n",
    "    self.gru2=nn.LSTM(input_size=32,hidden_size=32,batch_first=True)\n",
    "    self.gru3=nn.LSTM(input_size=64,hidden_size=32,batch_first=True)\n",
    "    self.gru4=nn.LSTM(input_size=96,hidden_size=32,batch_first=True)\n",
    "    self.gru_linear=nn.Linear(62,1)\n",
    "    self.flatten=nn.Flatten()\n",
    "    self.fc1=nn.Linear(32,1)\n",
    "    self.relu=nn.ReLU()\n",
    "\n",
    "  def forward(self,x):\n",
    "    x = x.squeeze()\n",
    "    print(x.shape)\n",
    "    x=self.block1(x)\n",
    "    print(x.shape)\n",
    "    x=self.block2(x)\n",
    "    print(x.shape)\n",
    "    x=self.block3(x)\n",
    "    print(x.shape)\n",
    "    x=x.permute(0,2,1)\n",
    "    print(x.shape)\n",
    "    gru_out1,_=self.gru1(x)\n",
    "    print(gru_out1.shape)\n",
    "    gru_out2,_=self.gru2(gru_out1)\n",
    "    print(gru_out2.shape)\n",
    "    gru_out=torch.cat([gru_out1, gru_out2],dim=2)\n",
    "    print(gru_out.shape)\n",
    "    gru_out3,_=self.gru3(gru_out)\n",
    "    print(gru_out3.shape)\n",
    "    gru_out=torch.cat([gru_out1, gru_out2, gru_out3],dim=2)\n",
    "    print(gru_out.shape)\n",
    "    linear_out=self.relu(self.gru_linear(gru_out.permute(0,2,1)))\n",
    "    print(linear_out.shape)\n",
    "    gru_out4,_=self.gru4(linear_out.permute(0,2,1))\n",
    "    print(gru_out4.shape)\n",
    "    x=self.flatten(gru_out4)\n",
    "    print(x.shape)\n",
    "    x=self.fc1(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "class LSTM(nn.Module):\n",
    "    def __init__(self, input_size, num_channels, hidden_units):\n",
    "        super().__init__()\n",
    "        self.num_channels = num_channels\n",
    "        self.hidden_units = hidden_units\n",
    "        self.num_layers = 2\n",
    "        self.input_size = input_size\n",
    "        self.inputsize_channels = num_channels * input_size\n",
    "\n",
    "        self.lstm = nn.LSTM(\n",
    "            input_size=num_channels,\n",
    "            hidden_size=hidden_units,\n",
    "            batch_first=True,\n",
    "            num_layers=self.num_layers\n",
    "        )\n",
    "\n",
    "        self.linear = nn.Linear(in_features=self.hidden_units, out_features=1)\n",
    "        self.enc = nn.Linear(in_features=self.inputsize_channels, out_features=19*num_channels)\n",
    "\n",
    "    def forward(self, x):\n",
    "        print(x.shape)\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.view(batch_size, x.shape[1]*x.shape[2])\n",
    "        print(x.shape)\n",
    "        x = self.enc(x)\n",
    "        print(x.shape)\n",
    "        x = x.view(batch_size, int(x.shape[-1]/self.num_channels), self.num_channels)\n",
    "        print(x.shape)\n",
    "        h0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        c0 = torch.zeros(self.num_layers, batch_size, self.hidden_units).requires_grad_()\n",
    "        lstm_out, (hn, _) = self.lstm(x, (h0, c0))\n",
    "        print(hn.shape)\n",
    "        out = self.linear(hn[0]).flatten()\n",
    "        print(lstm_out.shape)\n",
    "        #print(lstm_out.shape)\n",
    "        #lstm_out = lstm_out.view(batch_size,int(lstm_out.shape[-1]/19), 19)\n",
    "        #out = torch.sigmoid(out)\n",
    "        return lstm_out\n",
    "\n",
    "    \n",
    "class GraphConvolution(Module):\n",
    "    \"\"\"\n",
    "    Simple GCN layer, similar to https://arxiv.org/abs/1609.02907\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, in_features, out_features, bias=True):\n",
    "        super(GraphConvolution, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        #self.weight = Parameter(torch.FloatTensor(in_features, out_features))\n",
    "        #if bias:\n",
    "        #    self.bias = Parameter(torch.FloatTensor(out_features))\n",
    "        #else:\n",
    "        #    self.register_parameter('bias', None)\n",
    "        #self.reset_parameters()\n",
    "        \n",
    "        self.LSTM = LSTM(input_size=500, num_channels=19, hidden_units=128)\n",
    "        \n",
    "    #def reset_parameters(self):\n",
    "    #    stdv = 1. / math.sqrt(self.weight.size(1))\n",
    "    #    self.weight.data.uniform_(-stdv, stdv)\n",
    "    #    if self.bias is not None:\n",
    "    #        self.bias.data.uniform_(-stdv, stdv)\n",
    "\n",
    "    def forward(self, input, adj):\n",
    "        support = self.LSTM(input)\n",
    "        #print(adj.shape, support.shape)\n",
    "        #support = torch.mm(input, self.weight)\n",
    "        output = torch.matmul(adj, support)\n",
    "        #if self.bias is not None:\n",
    "        #    return output + self.bias\n",
    "        #else:\n",
    "        return output\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.__class__.__name__ + ' (' \\\n",
    "               + str(self.in_features) + ' -> ' \\\n",
    "               + str(self.out_features) + ')'\n",
    "    \n",
    "    \n",
    "    \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "class GCN(nn.Module):\n",
    "    def __init__(self, nfeat, nhid, nclass, dropout):\n",
    "        super(GCN, self).__init__()\n",
    "\n",
    "        self.gc1 = GraphConvolution(nfeat, nclass)\n",
    "        #self.gc2 = GraphConvolution(nhid, nclass)\n",
    "        self.dropout = dropout\n",
    "        self.flatten=nn.Flatten()\n",
    "        self.fc1=nn.Linear(2432,1)\n",
    "        self.relu=nn.ReLU()\n",
    "        \n",
    "    def forward(self, x, adj):\n",
    "        x = F.relu(self.gc1(x, adj))\n",
    "        x = F.dropout(x, self.dropout, training=self.training)\n",
    "        #x = self.gc2(x, adj)\n",
    "        #print(x.shape)\n",
    "        out = x.view(x.shape[0], x.shape[1]*x.shape[2])\n",
    "        out = self.fc1(out)\n",
    "        out = torch.sigmoid(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "id": "f2a3f968-7258-4486-a596-7322caf5cd0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 19, 500])\n",
      "torch.Size([3, 96, 250])\n",
      "torch.Size([3, 96, 125])\n",
      "torch.Size([3, 96, 62])\n",
      "torch.Size([3, 62, 96])\n",
      "torch.Size([3, 62, 32])\n",
      "torch.Size([3, 62, 32])\n",
      "torch.Size([3, 62, 64])\n",
      "torch.Size([3, 62, 32])\n",
      "torch.Size([3, 62, 96])\n",
      "torch.Size([3, 96, 1])\n",
      "torch.Size([3, 1, 32])\n",
      "torch.Size([3, 32])\n",
      "##########\n",
      "torch.Size([3, 19, 500])\n",
      "torch.Size([3, 9500])\n",
      "torch.Size([3, 361])\n",
      "torch.Size([3, 19, 19])\n",
      "torch.Size([2, 3, 32])\n",
      "torch.Size([3, 19, 32])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.0530, -0.0168, -0.0257,  ...,  0.0230, -0.0073, -0.0094],\n",
       "         [-0.0771, -0.0259, -0.0349,  ...,  0.0272, -0.0191, -0.0173],\n",
       "         [-0.0863, -0.0333, -0.0463,  ...,  0.0192, -0.0163, -0.0074],\n",
       "         ...,\n",
       "         [-0.1019, -0.0285, -0.0719,  ...,  0.0067, -0.0027,  0.0284],\n",
       "         [-0.1066, -0.0360, -0.0681,  ..., -0.0041, -0.0056,  0.0402],\n",
       "         [-0.1113, -0.0413, -0.0689,  ..., -0.0156, -0.0012,  0.0495]],\n",
       "\n",
       "        [[-0.0513, -0.0224, -0.0314,  ...,  0.0177, -0.0067, -0.0049],\n",
       "         [-0.0784, -0.0299, -0.0442,  ...,  0.0224, -0.0185, -0.0122],\n",
       "         [-0.0902, -0.0320, -0.0577,  ...,  0.0206, -0.0165, -0.0004],\n",
       "         ...,\n",
       "         [-0.1011, -0.0328, -0.0700,  ..., -0.0079, -0.0014,  0.0371],\n",
       "         [-0.1062, -0.0291, -0.0665,  ..., -0.0155, -0.0088,  0.0338],\n",
       "         [-0.1101, -0.0328, -0.0641,  ..., -0.0201, -0.0022,  0.0379]],\n",
       "\n",
       "        [[-0.0534, -0.0206, -0.0300,  ...,  0.0218, -0.0115, -0.0151],\n",
       "         [-0.0807, -0.0320, -0.0398,  ...,  0.0228, -0.0321, -0.0243],\n",
       "         [-0.0887, -0.0426, -0.0507,  ...,  0.0141, -0.0293, -0.0152],\n",
       "         ...,\n",
       "         [-0.1017, -0.0223, -0.0733,  ...,  0.0045,  0.0022,  0.0363],\n",
       "         [-0.1045, -0.0273, -0.0710,  ..., -0.0063, -0.0064,  0.0435],\n",
       "         [-0.1075, -0.0386, -0.0690,  ..., -0.0165, -0.0059,  0.0527]]],\n",
       "       grad_fn=<TransposeBackward0>)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ch_net = ChronoNet(19)\n",
    "ls = LSTM(500, 19, 32)\n",
    "ch_net(features)\n",
    "print(\"##########\")\n",
    "ls(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a88385ea-7be3-4adb-8dfa-aeb32d02e54f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "id": "7d7fa06e-4769-45a6-be07-9c44c845a5ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "#adj.shape, features.shape\n",
    "features = torch.rand(3, 19, 500)\n",
    "adj = torch.rand(3, 19, 19)\n",
    "aa = torch.matmul(adj, features)\n",
    "#aa.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "id": "d64e02fd-4c35-417a-8d52-019b003dd343",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(88500, 19, 500)"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "id": "ba834795-c35e-468d-8a04-d4a7ca0169af",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 88500/88500 [42:04<00:00, 35.06it/s]\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import mstats\n",
    "from tqdm import tqdm\n",
    "\"\"\"\n",
    "adj = []\n",
    "for feat in train_features:\n",
    "    for ch in feat:\n",
    "        pcorr = mstats.pearsonr([1, 2, 3, 4, 5], [10, 9, 2.5, 6, 4])[0]\n",
    "        adj.append(pcorr)\n",
    "\"\"\" \n",
    "\n",
    "adj=[]\n",
    "for f_t in tqdm(train_features):\n",
    "    matrix = []\n",
    "    for i in range(19):\n",
    "        row = []\n",
    "        for j in range(19):\n",
    "            res = mstats.pearsonr(f_t[i, :], f_t[j, :])\n",
    "            row.append(res[0])\n",
    "        matrix.append(row)\n",
    "        #matrix = np.array(matrix)\n",
    "    adj.append(matrix)\n",
    "adj=np.array(adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "id": "0db6ed41-0b10-4df3-ba2b-409681ec3c6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████| 29500/29500 [14:04<00:00, 34.94it/s]\n"
     ]
    }
   ],
   "source": [
    "val_adj = []\n",
    "for f_t in tqdm(val_features):\n",
    "    matrix = []\n",
    "    for i in range(19):\n",
    "        row = []\n",
    "        for j in range(19):\n",
    "            res = mstats.pearsonr(f_t[i, :], f_t[j, :])\n",
    "            row.append(res[0])\n",
    "        matrix.append(row)\n",
    "        #matrix = np.array(matrix)\n",
    "    val_adj.append(matrix)\n",
    "val_adj = np.array(val_adj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "id": "c9be2e9d-2c79-49e7-b94d-d4a66a9c82a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([88500, 19, 19]),\n",
       " torch.Size([88500, 19, 500]),\n",
       " torch.Size([88500]),\n",
       " (29500, 19, 500),\n",
       " torch.Size([29500, 19, 19]),\n",
       " (29500,))"
      ]
     },
     "execution_count": 242,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "adj.shape, train_features.shape, train_labels.shape, val_features.shape, val_adj.shape , val_labels.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "id": "8d1eafc5-74d7-4508-b460-422a5f9b4a4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 19, 128])\n",
      "torch.Size([3, 19, 19]) torch.Size([3, 19, 128])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 19, 128])"
      ]
     },
     "execution_count": 186,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "output = model(features, adj)\n",
    "output.shape\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "id": "dd545f6c-901a-420b-bb8a-2c25bee229fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#train_features = torch.Tensor(train_features).float().to(device)\n",
    "#train_labels = torch.Tensor(train_labels).float().to(device)\n",
    "adj = torch.Tensor(adj).float().to(device)\n",
    "data = torch.utils.data.TensorDataset(train_features, adj, train_labels)\n",
    "data_iter = torch.utils.data.DataLoader(data, 128, shuffle=True)\n",
    "\n",
    "val_adj = torch.Tensor(val_adj).float().to(device)\n",
    "val_labels = torch.Tensor(val_labels).float().to(device)\n",
    "val_features = torch.Tensor(val_features).float().to(device)\n",
    "data = torch.utils.data.TensorDataset(val_features, val_adj, val_labels)\n",
    "val_data_iter = torch.utils.data.DataLoader(data, 128, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "id": "5fa90b13-a423-45ab-b629-797048837177",
   "metadata": {},
   "outputs": [],
   "source": [
    "def cal_accuracy(model, data_iter):\n",
    "    ytrue = []\n",
    "    ypreds = []\n",
    "    with torch.no_grad():\n",
    "        for x, x_adj, y in data_iter:\n",
    "            yhat = model(x, x_adj)\n",
    "            yhat = [0 if i<0.5 else 1 for i in yhat]\n",
    "            ytrue.extend(list(y.numpy()))\n",
    "            ypreds.extend(yhat)\n",
    "\n",
    "    return (accuracy_score(ytrue, ypreds), \n",
    "            confusion_matrix(ytrue, ypreds), \n",
    "            precision_score(ytrue, ypreds), \n",
    "            recall_score(ytrue, ypreds),\n",
    "            f1_score(ytrue, ypreds))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "0828b0c4-ea8d-4fd1-b64e-fa2f94d91bf7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Model....\n",
      "started training\n",
      "Epoch 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [02:04<00:00,  5.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6674588418248072 ,Train Accuracy:  0.7116497175141243 ,F1:  0.6706935354528286 ,Precision:  0.7857793153678078 ,Recall:  0.5846779661016949\n",
      "Epoch 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [01:57<00:00,  5.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.6262357856841446 ,Train Accuracy:  0.7484519774011299 ,F1:  0.7045285425746785 ,Precision:  0.8537385431741438 ,Recall:  0.6008361581920904\n",
      "Epoch 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [02:00<00:00,  5.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.611456700487633 ,Train Accuracy:  0.7816836158192091 ,F1:  0.7523138059223177 ,Precision:  0.8691926828545169 ,Recall:  0.6631412429378531\n",
      "Epoch 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [01:59<00:00,  5.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5996646712281112 ,Train Accuracy:  0.8080790960451978 ,F1:  0.789773009486912 ,Precision:  0.8753851232394366 ,Recall:  0.7183502824858757\n",
      "Epoch 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████| 692/692 [01:56<00:00,  5.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5894182781780386 ,Train Accuracy:  0.8257740112994351 ,F1:  0.8052003895233398 ,Precision:  0.9140010337104462 ,Recall:  0.7188926553672317\n"
     ]
    }
   ],
   "source": [
    "print(\"Training Model....\")\n",
    "n_chans = 19\n",
    "DEVICE = torch.device(\"cpu\")\n",
    "# Model and optimizer\n",
    "gcn = GCN(nfeat=features.shape[1], nhid=4, nclass=2, dropout=0.5)\n",
    "gcn.to(DEVICE)\n",
    "loss_func = nn.BCEWithLogitsLoss()\n",
    "optimizer = torch.optim.Adam(gcn.parameters(), lr=1e-3)\n",
    "NUM_EPOCHS = 5\n",
    "BATCH_SIZE = 128\n",
    "\n",
    "print(\"started training\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    print(\"Epoch\", epoch) \n",
    "    loss_sum, n = 0.0, 0\n",
    "    model.train()\n",
    "    for t, (x, adj_x ,y) in enumerate(tqdm(data_iter)):\n",
    "        y_pred = gcn(x, adj_x)\n",
    "        y_pred = y_pred.squeeze()\n",
    "        #print(y_pred.shape, y.shape)\n",
    "        loss = loss_func(y_pred, y)\n",
    "        loss.backward()\n",
    "        loss_sum += loss.item()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "    #val_loss = evaluate_model(gcn, loss_func, test_iter)\n",
    "    print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
    "        cal_accuracy(gcn, data_iter)[0], \",F1: \", \n",
    "        cal_accuracy(gcn, data_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(gcn, data_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(gcn, data_iter)[3])\n",
    "    print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[0], \",F1: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "id": "8da4cb85-1da6-49da-8dd0-b54d5a91b842",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train loss: 0.5894182781780386 ,Train Accuracy:  0.8250169491525424 ,F1:  0.8050405086009681 ,Precision:  0.914440039073723 ,Recall:  0.7181694915254238\n"
     ]
    }
   ],
   "source": [
    "print(\"Train loss:\", loss_sum / (t+1), \",Train Accuracy: \", \n",
    "        cal_accuracy(gcn, data_iter)[0], \",F1: \", \n",
    "        cal_accuracy(gcn, data_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(gcn, data_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(gcn, data_iter)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "id": "bce680eb-4dd0-459d-a128-11bb2fe1439c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Val loss: 0.5963385198023412 , Val Accuracy:  0.698135593220339 ,F1:  0.6354681816324028 ,Precision:  0.8046131568059578 ,Recall:  0.5246779661016949\n"
     ]
    }
   ],
   "source": [
    "print(\"Val loss:\", val_loss, \", Val Accuracy: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[0], \",F1: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[4], \",Precision: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[2], \",Recall: \", \n",
    "        cal_accuracy(gcn, val_data_iter)[3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49dfbdf6-78a6-442d-84e0-63ee55a31355",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "    \n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "\n",
    "\n",
    "def encode_onehot(labels):\n",
    "    classes = set(labels)\n",
    "    classes_dict = {c: np.identity(len(classes))[i, :] for i, c in\n",
    "                    enumerate(classes)}\n",
    "    labels_onehot = np.array(list(map(classes_dict.get, labels)),\n",
    "                             dtype=np.int32)\n",
    "    return labels_onehot\n",
    "\n",
    "\n",
    "def normalize(mx):\n",
    "    \"\"\"Row-normalize sparse matrix\"\"\"\n",
    "    rowsum = np.array(mx.sum(1))\n",
    "    r_inv = np.power(rowsum, -1).flatten()\n",
    "    r_inv[np.isinf(r_inv)] = 0.\n",
    "    r_mat_inv = sp.diags(r_inv)\n",
    "    mx = r_mat_inv.dot(mx)\n",
    "    return mx\n",
    "\n",
    "\n",
    "def accuracy(output, labels):\n",
    "    preds = output.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(labels).double()\n",
    "    correct = correct.sum()\n",
    "    return correct / len(labels)\n",
    "\n",
    "\n",
    "def sparse_mx_to_torch_sparse_tensor(sparse_mx):\n",
    "    \"\"\"Convert a scipy sparse matrix to a torch sparse tensor.\"\"\"\n",
    "    sparse_mx = sparse_mx.tocoo().astype(np.float32)\n",
    "    indices = torch.from_numpy(\n",
    "        np.vstack((sparse_mx.row, sparse_mx.col)).astype(np.int64))\n",
    "    values = torch.from_numpy(sparse_mx.data)\n",
    "    shape = torch.Size(sparse_mx.shape)\n",
    "    return torch.sparse.FloatTensor(indices, values, shape)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
